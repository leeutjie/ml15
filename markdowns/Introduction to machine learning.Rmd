---
title: "Introduction to Machine Learning"
author: "Louis Becker"
date: "March 21, 2018"
output: html_document
---



This markdown is meant to complement the available slides, and will have the odd link here and there to some related material. The purpose is not to delve into theoretical detail here, but to state the bare minimum for reference.

# 1. Introduction

Resources:

* [R-bloggers Course](https://www.r-bloggers.com/in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos/)
* [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)
* [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)

## Examples
Links to some stuff mentioned:

* [FiveThirtyEight](http://fivethirtyeight.com/)
* [Watson](https://www.ibm.com/watson/)

Examples of statistical learning problems:

* Identify the risk factors for prostate cancer.
* Classify a recorded phoneme based on a log-periodogram.
* Predict whether someone will have a heart attack on the basis of demographic, diet and clinical measurements.
* Customize an email spam detection system.
* Identify the numbers in a handwritten zip code.
* Classify a tissue sample into one of several cancer classes, based on a gene expression profile.
* Establish the relationship between salary and demographic variables in population survey data.
* Classify the pixels in a LANDSAT image, by usage.

## The Supervised Learning Problem

Where do we start? With definitions, obviously:

* *Y* - Outcome variable (dependent variable, response variable, target variabel, etc.)
* ***X*** - vector of *p* predictor measurements (inputs, regressors, covariates, features, independent variables)
* *Regression Problem* - Problem where *Y* is defeined as a **quantitative** variable
* *Classification Problem* - where *Y* is defined as a **qualitative** or **indicator** variable, taking values in a finite, unordered set
* *Training Data* - Observations *(x~1~,y~1~),...,(x~N~,y~N~)* that are measurements.
* The objecticve of supervised learning is to use training data for
    + Predicting unseen test cases
    + Understanding which inputs affect the outcome and how
    + Assess the quality of predictions and inferences

## So what is unsupervised learning, then?
* No outcome variable, just a set of predictors (features) measured on a set of samples.
* The objective is more fuzzy: 
    + Find groups of samples that behave similarly 
    + Find features that behave similarly 
    + Find linear combinations of features with the most variation
* It is difficult to know how well you're doing
* It is different from supervised learning, but can be useful as a pre-processing step for supervised learning

# 2. Statistical Learning


We state our model such that our outcome $Y$ is a function of inputs $X$ with some stochastic component $\epsilon$ that captures errors and other discrepancies:

$$
Y = f(X) + \epsilon
$$

If our function, $f$, is good enough, we can make predictions of $Y$ given specific values of the predictors (where $X=x$). The question can then be asked as to what is a good value for $f(X)$ for any value assigned to $X$? It is likely in any data that there will be multiple $Y$ outcomes for a particular value of $X$. A good approximation of $Y$ would then be its conditional expectation:

$$
Y = f(X) = E(Y|X = x)
$$
This is commonly known as a regression function. A regression function can also be defined for a vector *$X$*:

$$
f(x) = f(x_1, x_2, x_3) = E(Y|X_1 = x_1, X_2 = x_2, X_3 = x_3)
$$

We define the *optimal* predictor of $Y$ as the function, $f(x) = E(Y|X=x)$, that minimises the mean squared prediction error
$$
E[(Y-d(X))^2|X=x]
$$
over all functions $g$ for all $X=x$. Now, let's turn our attention to the error, $\epsilon$. This component can be split into two types of error for any estimate $\hat{f}(x)$ of $f(x)$, namely the reducible error and the non-reducible error:
$$
E[(Y-d(X))^2|X=x] = [] + Var(\epsilon)
$$






